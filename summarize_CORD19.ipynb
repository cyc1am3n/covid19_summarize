{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import easydict\n",
    "\n",
    "from finetune import SummarizationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"adam_epsilon\":1e-08,\n",
    "    \"cache_dir\":'',\n",
    "    \"config_name\":'',\n",
    "    \"data_dir\":'./data/cnn_dm/cnn_dm',\n",
    "    \"do_lower_case\":False,\n",
    "    \"do_predict\":False,\n",
    "    \"do_train\":True,\n",
    "    \"eval_batch_size\":4,\n",
    "    \"fp16\":False,\n",
    "    \"fp16_opt_level\":'O1',\n",
    "    \"gradient_accumulation_steps\":1,\n",
    "    \"learning_rate\":3e-05,\n",
    "    \"max_grad_norm\":1.0,\n",
    "    \"max_source_length\":1024,\n",
    "    \"max_target_length\":56,\n",
    "    \"model_name_or_path\":'t5-small',\n",
    "    \"model_type\":'t5',\n",
    "    \"n_gpu\":1,\n",
    "    \"n_tpu_cores\":0,\n",
    "    \"num_train_epochs\":3,\n",
    "    \"output_dir\":'./output',\n",
    "    \"seed\":42,\n",
    "    \"server_ip\":'',\n",
    "    \"server_port\":'',\n",
    "    \"tokenizer_name\":'',\n",
    "    \"train_batch_size\":4,\n",
    "    \"warmup_steps\":0,\n",
    "    \"weight_decay\":0.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\a5c3f374b1e8ac4f5e37dda9b0176d2b7e33a7de6b570ef6b8490f35240b2cf0.04cbd562eb01c8cec40cf5fefcc83823e6ba146bd2a758202a770beb7d80bb5d\n",
      "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in T5ForConditionalGeneration: ['encoder.block.0.layer.0.layer_norm.bias', 'encoder.block.0.layer.1.layer_norm.bias', 'encoder.block.1.layer.0.layer_norm.bias', 'encoder.block.1.layer.1.layer_norm.bias', 'encoder.block.2.layer.0.layer_norm.bias', 'encoder.block.2.layer.1.layer_norm.bias', 'encoder.block.3.layer.0.layer_norm.bias', 'encoder.block.3.layer.1.layer_norm.bias', 'encoder.block.4.layer.0.layer_norm.bias', 'encoder.block.4.layer.1.layer_norm.bias', 'encoder.block.5.layer.0.layer_norm.bias', 'encoder.block.5.layer.1.layer_norm.bias', 'encoder.final_layer_norm.bias', 'decoder.block.0.layer.0.layer_norm.bias', 'decoder.block.0.layer.1.layer_norm.bias', 'decoder.block.0.layer.2.layer_norm.bias', 'decoder.block.1.layer.0.layer_norm.bias', 'decoder.block.1.layer.1.layer_norm.bias', 'decoder.block.1.layer.2.layer_norm.bias', 'decoder.block.2.layer.0.layer_norm.bias', 'decoder.block.2.layer.1.layer_norm.bias', 'decoder.block.2.layer.2.layer_norm.bias', 'decoder.block.3.layer.0.layer_norm.bias', 'decoder.block.3.layer.1.layer_norm.bias', 'decoder.block.3.layer.2.layer_norm.bias', 'decoder.block.4.layer.0.layer_norm.bias', 'decoder.block.4.layer.1.layer_norm.bias', 'decoder.block.4.layer.2.layer_norm.bias', 'decoder.block.5.layer.0.layer_norm.bias', 'decoder.block.5.layer.1.layer_norm.bias', 'decoder.block.5.layer.2.layer_norm.bias', 'decoder.final_layer_norm.bias']\n"
     ]
    }
   ],
   "source": [
    "trainer = SummarizationTrainer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./output\\\\checkpointepoch=0.ckpt',\n",
       " './output\\\\checkpointepoch=1.ckpt',\n",
       " './output\\\\checkpointepoch=2.ckpt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finetuned weight\n",
    "checkpoints = list(sorted(glob.glob(os.path.join(args.output_dir, \"checkpointepoch=*.ckpt\"), recursive=True)))\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = './data/CORD19/abstract_ndq.txt'\n",
    "\n",
    "source_lns = [x.rstrip() for x in open(text_input, encoding=\"UTF8\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [diff]:  227\n",
      "13 [diff]:  777\n",
      "15 [diff]:  300\n",
      "30 [diff]:  248\n",
      "35 [diff]:  1263\n",
      "128 [diff]:  255\n",
      "130 [diff]:  231\n",
      "131 [diff]:  34\n",
      "35798\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for i in range(150):\n",
    "    dct = trainer.tokenizer.batch_encode_plus([source_lns[i]], max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "    input_ids = dct[\"input_ids\"].to(device)\n",
    "    dec = [trainer.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in input_ids][0]\n",
    "    src = source_lns[i]\n",
    "    if len(src) - len(dec) > 15:\n",
    "        print(i, end=\" \")\n",
    "        print(\"[diff]: \", len(src)-len(dec))\n",
    "        del source_lns[i]\n",
    "\n",
    "print(len(source_lns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lns = source_lns[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [02:21<00:00, 10.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# From Not Finetuned Model\n",
    "trainer.model.to(device)\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 140\n",
    "min_length = 55\n",
    "\n",
    "output_file = Path('./data/CORD19/no_ft_generated_summary.txt').open(\"w\", encoding=\"UTF8\")\n",
    "for batch in tqdm(list(chunks(source_lns, batch_size))):\n",
    "    batch = [\"summarize: \" + text for text in batch]\n",
    "    \n",
    "    dct = trainer.tokenizer.batch_encode_plus(batch, max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "    input_ids = dct[\"input_ids\"].to(device)\n",
    "    attention_mask = dct[\"attention_mask\"].to(device)\n",
    "    \n",
    "    summaries = trainer.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        max_length=max_length + 2,  # +2 from original because we start at step=1 and stop before max_length\n",
    "        min_length=min_length + 1,  # +1 from original because we start at step=1\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    dec = [trainer.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
    "    \n",
    "    for hypothesis in dec:\n",
    "        output_file.write('.'.join(hypothesis.split('.')[:-1]) + '.' + \"\\n\")\n",
    "        output_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\a5c3f374b1e8ac4f5e37dda9b0176d2b7e33a7de6b570ef6b8490f35240b2cf0.04cbd562eb01c8cec40cf5fefcc83823e6ba146bd2a758202a770beb7d80bb5d\n",
      "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in T5ForConditionalGeneration: ['encoder.block.0.layer.0.layer_norm.bias', 'encoder.block.0.layer.1.layer_norm.bias', 'encoder.block.1.layer.0.layer_norm.bias', 'encoder.block.1.layer.1.layer_norm.bias', 'encoder.block.2.layer.0.layer_norm.bias', 'encoder.block.2.layer.1.layer_norm.bias', 'encoder.block.3.layer.0.layer_norm.bias', 'encoder.block.3.layer.1.layer_norm.bias', 'encoder.block.4.layer.0.layer_norm.bias', 'encoder.block.4.layer.1.layer_norm.bias', 'encoder.block.5.layer.0.layer_norm.bias', 'encoder.block.5.layer.1.layer_norm.bias', 'encoder.final_layer_norm.bias', 'decoder.block.0.layer.0.layer_norm.bias', 'decoder.block.0.layer.1.layer_norm.bias', 'decoder.block.0.layer.2.layer_norm.bias', 'decoder.block.1.layer.0.layer_norm.bias', 'decoder.block.1.layer.1.layer_norm.bias', 'decoder.block.1.layer.2.layer_norm.bias', 'decoder.block.2.layer.0.layer_norm.bias', 'decoder.block.2.layer.1.layer_norm.bias', 'decoder.block.2.layer.2.layer_norm.bias', 'decoder.block.3.layer.0.layer_norm.bias', 'decoder.block.3.layer.1.layer_norm.bias', 'decoder.block.3.layer.2.layer_norm.bias', 'decoder.block.4.layer.0.layer_norm.bias', 'decoder.block.4.layer.1.layer_norm.bias', 'decoder.block.4.layer.2.layer_norm.bias', 'decoder.block.5.layer.0.layer_norm.bias', 'decoder.block.5.layer.1.layer_norm.bias', 'decoder.block.5.layer.2.layer_norm.bias', 'decoder.final_layer_norm.bias']\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [03:23<00:00, 15.64s/it]\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\a5c3f374b1e8ac4f5e37dda9b0176d2b7e33a7de6b570ef6b8490f35240b2cf0.04cbd562eb01c8cec40cf5fefcc83823e6ba146bd2a758202a770beb7d80bb5d\n",
      "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in T5ForConditionalGeneration: ['encoder.block.0.layer.0.layer_norm.bias', 'encoder.block.0.layer.1.layer_norm.bias', 'encoder.block.1.layer.0.layer_norm.bias', 'encoder.block.1.layer.1.layer_norm.bias', 'encoder.block.2.layer.0.layer_norm.bias', 'encoder.block.2.layer.1.layer_norm.bias', 'encoder.block.3.layer.0.layer_norm.bias', 'encoder.block.3.layer.1.layer_norm.bias', 'encoder.block.4.layer.0.layer_norm.bias', 'encoder.block.4.layer.1.layer_norm.bias', 'encoder.block.5.layer.0.layer_norm.bias', 'encoder.block.5.layer.1.layer_norm.bias', 'encoder.final_layer_norm.bias', 'decoder.block.0.layer.0.layer_norm.bias', 'decoder.block.0.layer.1.layer_norm.bias', 'decoder.block.0.layer.2.layer_norm.bias', 'decoder.block.1.layer.0.layer_norm.bias', 'decoder.block.1.layer.1.layer_norm.bias', 'decoder.block.1.layer.2.layer_norm.bias', 'decoder.block.2.layer.0.layer_norm.bias', 'decoder.block.2.layer.1.layer_norm.bias', 'decoder.block.2.layer.2.layer_norm.bias', 'decoder.block.3.layer.0.layer_norm.bias', 'decoder.block.3.layer.1.layer_norm.bias', 'decoder.block.3.layer.2.layer_norm.bias', 'decoder.block.4.layer.0.layer_norm.bias', 'decoder.block.4.layer.1.layer_norm.bias', 'decoder.block.4.layer.2.layer_norm.bias', 'decoder.block.5.layer.0.layer_norm.bias', 'decoder.block.5.layer.1.layer_norm.bias', 'decoder.block.5.layer.2.layer_norm.bias', 'decoder.final_layer_norm.bias']\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [03:21<00:00, 15.48s/it]\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.75aa1adfaef9acd7cb4b5e3aca5831c9b362b5940c2cc86a1f2dc353283117b0\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin from cache at C:\\Users\\Daeyoung\\.cache\\torch\\transformers\\a5c3f374b1e8ac4f5e37dda9b0176d2b7e33a7de6b570ef6b8490f35240b2cf0.04cbd562eb01c8cec40cf5fefcc83823e6ba146bd2a758202a770beb7d80bb5d\n",
      "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in T5ForConditionalGeneration: ['encoder.block.0.layer.0.layer_norm.bias', 'encoder.block.0.layer.1.layer_norm.bias', 'encoder.block.1.layer.0.layer_norm.bias', 'encoder.block.1.layer.1.layer_norm.bias', 'encoder.block.2.layer.0.layer_norm.bias', 'encoder.block.2.layer.1.layer_norm.bias', 'encoder.block.3.layer.0.layer_norm.bias', 'encoder.block.3.layer.1.layer_norm.bias', 'encoder.block.4.layer.0.layer_norm.bias', 'encoder.block.4.layer.1.layer_norm.bias', 'encoder.block.5.layer.0.layer_norm.bias', 'encoder.block.5.layer.1.layer_norm.bias', 'encoder.final_layer_norm.bias', 'decoder.block.0.layer.0.layer_norm.bias', 'decoder.block.0.layer.1.layer_norm.bias', 'decoder.block.0.layer.2.layer_norm.bias', 'decoder.block.1.layer.0.layer_norm.bias', 'decoder.block.1.layer.1.layer_norm.bias', 'decoder.block.1.layer.2.layer_norm.bias', 'decoder.block.2.layer.0.layer_norm.bias', 'decoder.block.2.layer.1.layer_norm.bias', 'decoder.block.2.layer.2.layer_norm.bias', 'decoder.block.3.layer.0.layer_norm.bias', 'decoder.block.3.layer.1.layer_norm.bias', 'decoder.block.3.layer.2.layer_norm.bias', 'decoder.block.4.layer.0.layer_norm.bias', 'decoder.block.4.layer.1.layer_norm.bias', 'decoder.block.4.layer.2.layer_norm.bias', 'decoder.block.5.layer.0.layer_norm.bias', 'decoder.block.5.layer.1.layer_norm.bias', 'decoder.block.5.layer.2.layer_norm.bias', 'decoder.final_layer_norm.bias']\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [03:23<00:00, 15.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# From finetuned model\n",
    "for i, ckpt in enumerate(checkpoints):\n",
    "    trainer = SummarizationTrainer.load_from_checkpoint(checkpoints[i])\n",
    "    trainer.model.to(\"cuda\")\n",
    "    \n",
    "    output_file = Path(f\"./data/CORD19/ft_ckpt_{str(i)}_generated_summary.txt\").open(\"w\", encoding=\"UTF8\")\n",
    "    for batch in tqdm(list(chunks(source_lns, batch_size))):\n",
    "        batch = [\"summarize: \" + text for text in batch]\n",
    "\n",
    "        dct = trainer.tokenizer.batch_encode_plus(batch, max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "        input_ids = dct[\"input_ids\"].to(device)\n",
    "        attention_mask = dct[\"attention_mask\"].to(device)\n",
    "\n",
    "        summaries = trainer.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            max_length=max_length + 2,  # +2 from original because we start at step=1 and stop before max_length\n",
    "            min_length=min_length + 1,  # +1 from original because we start at step=1\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        dec = [trainer.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
    "\n",
    "        for hypothesis in dec:\n",
    "            output_file.write('.'.join(hypothesis.split('.')[:-1]) + '.' + \"\\n\")\n",
    "            output_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sum",
   "language": "python",
   "name": "sum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
